name: GradientBoostingRanker
trainer: XGBoostTrainer

# Number of gradient boosted trees. Equivalent to number of boosting rounds.
n_estimators: 150

# Maximum tree depth for base learners.
max_depth: 5

# Boosting learning rate (xgb's "eta")
learning_rate: 0.1

# The degree of verbosity. Valid values are 0 (silent) - 3 (debug).
verbosity: 3

# Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).
# rank:pairwise
# rank:ndcg
# rank:map
objective: rank:pairwise

# (min_split_loss) Minimum loss reduction required to make a further partition on a leaf node of the tree.
gamma: 0

# Minimum sum of instance weight (hessian) needed in a child.
min_child_weight: 1

# L1 regularization term on weights (xgb's alpha).
reg_alpha: 0

# L2 regularization term on weights (xgb's lambda).
reg_lambda: 0.1

subsample: 1

colsample_bytree: 1

grow_policy: depthwise

# Random number seed.
random_state: 7
