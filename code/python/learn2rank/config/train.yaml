defaults:
  - hydra: train
  - problem: knapsack
  - model: GradientBoostingRanker
  - optimizer: adam
  - run: default
  - wandb: default
  - _self_

# Use optuna to tune the hyperparameters of the XGBoost model
# Save models, predictions and results
save: 1

# Type of ranking task
# point_regress, pair_rank, pair_rank_all, pair_rank_all_context, multitask
task: pair_rank

out_path:
  cc: /home/rahulpat/scratch/l2o_resources
  desktop: /home/rahul/Documents/projects/multiobjective_cp2016
  laptop: /home/rahul/Documents/PhD/projects/multiobjective_cp2016

res_path:
  cc: /home/rahulpat/scratch/l2o_resources/
  desktop: /home/rahul/Documents/projects/multiobjective_cp2016/resources
  laptop: /home/rahul/Documents/PhD/projects/multiobjective_cp2016/resources

machine: desktop

dataset_path:
  point_regress: ${res_path[${machine}]}/datasets/${problem.name}/${problem.name}_dataset_${task}.pkl
  pair_rank: ${res_path[${machine}]}/datasets/${problem.name}
  pair_rank_all: ${res_path[${machine}]}/datasets/${problem.name}
  pair_rank_all_context: ${res_path[${machine}]}/datasets/${problem.name}

dataset:
  path: ${dataset_path[${task}]}
  fused: 0

# Load configs from this path
tuned_model:

case: 0
#output_dir: ${hydra:runtime.output_dir}

hydra:
  run:
    dir: ${out_path[${machine}]}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}_${case}
