defaults:
  - hydra: train
  - problem: knapsack
  - model: GradientBoostingRanker
  - optimizer: adam
  - run: default
  - wandb: default
  - _self_

# Use optuna to tune the hyperparameters of the XGBoost model
# TRAIN | TUNE
mode: TRAIN
# INFO | CRITICAL
logging_level: INFO
# Save models, predictions and results
save: 1

# Type of ranking task
# point_regress, pair_svmrank, pair_xgbrank, multitask
# pair_xgbrank_all
task: pair_xgbrank_all

out_path:
  cc: /home/rahulpat/scratch/l2o_resources
  desktop: /home/rahul/Documents/projects/multiobjective_cp2016
  laptop: /home/rahul/Documents/PhD/projects/multiobjective_cp2016

res_path:
  cc: /home/rahulpat/scratch/l2o_resources/
  desktop: /home/rahul/Documents/projects/multiobjective_cp2016/resources
  laptop: /home/rahul/Documents/PhD/projects/multiobjective_cp2016/resources

machine: desktop

dataset_path:
  point_regress: ${res_path[${machine}]}/datasets/${problem.name}/${problem.name}_dataset_${task}.pkl
  pair_svmrank: ${res_path[${machine}]}/datasets/${problem.name}
  pair_xgbrank: ${res_path[${machine}]}/datasets/${problem.name}
  pair_xgbrank_all: ${res_path[${machine}]}/datasets/${problem.name}

dataset:
  path: ${dataset_path[${task}]}
  fused: 1

case: 0
#output_dir: ${hydra:runtime.output_dir}

hydra:
  run:
    dir: ${out_path[${machine}]}/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}_${case}